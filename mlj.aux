\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{something}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section*.1}}
\newlabel{intro}{{1}{1}{Introduction\relax }{section*.1}{}}
\citation{something}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Feature discovery with relational learning. If we knew feature F1, it is easy to construct a model for active molecules using any machine learning program (rule at the bottom). What we are talking about here is discovering the definition of F1 (box on the right), given relational descriptions of the molecules m1--m4. Once done, we may be able to construct better models for the data.}}{2}{figure.1}}
\newlabel{fig:features}{{1}{2}{Feature discovery with relational learning. If we knew feature F1, it is easy to construct a model for active molecules using any machine learning program (rule at the bottom). What we are talking about here is discovering the definition of F1 (box on the right), given relational descriptions of the molecules m1--m4. Once done, we may be able to construct better models for the data}{figure.1}{}}
\citation{Larson_77}
\@writefile{toc}{\contentsline {section}{\numberline {2}Consensus-based Feature Selection for ILP Applications}{3}{figure.1}}
\newlabel{sec:example}{{2}{3}{Consensus-based Feature Selection for ILP Applications\relax }{figure.1}{}}
\citation{Specia_09}
\citation{l1Reg,cvpr2007,nips2004}
\citation{JoshiRS08,Amrita12,NageshRCKDB12,ChalamallaNSR08,SpeciaSJRN09,RamakrishnanJBS07,SpeciaSRN06}
\citation{JawanpuriaNR11,NairSRK12}
\citation{pei2004,subseqGap2006,bitSpade,antunes2003,han2005,rakesh1995,han2004,gehrke2002,rastogi1999}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The train spotting dataset. There are two sets of trains: Eastbound and Westbound. Descriptors for the trains include: number, shape and lengths of the car, shape of load, and so on. The task is to determine decision rules to distinguish between them. }}{4}{figure.2}}
\newlabel{trains}{{2}{4}{The train spotting dataset. There are two sets of trains: Eastbound and Westbound. Descriptors for the trains include: number, shape and lengths of the car, shape of load, and so on. The task is to determine decision rules to distinguish between them. \relax }{figure.2}{}}
\citation{Du_01a}
\citation{Du_01b}
\citation{Vaidya_02}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{5}{figure.2}}
\newlabel{sec:related}{{3}{5}{Related Work\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Feature Discovery}{5}{figure.2}}
\newlabel{sec:ilp}{{4}{5}{Feature Discovery\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Problem Statement}{5}{figure.2}}
\newlabel{sec:probdesc}{{5}{5}{Problem Statement\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Distributed Feature Estimation Algorithm}{6}{figure.2}}
\newlabel{sec:alg}{{6}{6}{Distributed Feature Estimation Algorithm\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Building Blocks: Distributed Scalar Product Computation}{6}{figure.2}}
\newlabel{sec:dotProd}{{6.1}{6}{Building Blocks: Distributed Scalar Product Computation\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Assumptions}{6}{figure.2}}
\newlabel{assum}{{6.2}{6}{Assumptions\relax }{figure.2}{}}
\citation{Kempe_03}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Distributed Feature Estimation}}{7}{figure.2}}
\newlabel{alg:fs}{{1}{7}{Algorithm Description\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Algorithm Description}{7}{figure.2}}
\newlabel{algo}{{6.3}{7}{Algorithm Description\relax }{figure.2}{}}
\citation{Varga_62}
\citation{Varga_62}
\citation{Varga_62}
\citation{Cybenko_89}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces State Transition Probability between two sites $S_i$ and $S_j$}}{8}{figure.3}}
\newlabel{stateTrans}{{3}{8}{State Transition Probability between two sites $S_i$ and $S_j$\relax }{figure.3}{}}
\citation{Tsitsiklis_86}
\newlabel{s0}{{4(a)}{9}{Subfigure 4(a)\relax }{figure.3}{}}
\newlabel{sub@s0}{{(a)}{9}{Subfigure 4(a)\relax }{figure.3}{}}
\newlabel{s1}{{4(b)}{9}{Subfigure 4(b)\relax }{figure.3}{}}
\newlabel{sub@s1}{{(b)}{9}{Subfigure 4(b)\relax }{figure.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Site 1}}}{9}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Site 2}}}{9}{figure.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Convergence}{9}{figure.3}}
\newlabel{sec:algterm}{{6.4}{9}{Convergence\relax }{figure.3}{}}
\citation{Boyd_04}
\citation{Hyers_52}
\citation{Pales_02}
\citation{Pales_02}
\citation{Hyers_52}
\newlabel{defn:1}{{1}{10}{Convergence\relax }{figure.3}{}}
\newlabel{defn:2}{{2}{10}{Convergence\relax }{figure.3}{}}
\citation{bottou-98x,Benveniste_90,Bertsekas_97}
\citation{Bottou_05}
\newlabel{localOpt1}{{1}{11}{Convergence\relax }{figure.3}{}}
\newlabel{localOpt2}{{2}{11}{Convergence\relax }{figure.3}{}}
\citation{Bottou_05}
\citation{lecun-98a}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Contour of the local and global optimization at a site. The brown concentric circles indicate the perturbation of the local cost to learn the global cost. The path traced by the yellow ochre colored circles indicate a possible optimization path if only the local cost were considered. The green colored circles show the path if the global cost were to be optimized. }}{12}{figure.4}}
\newlabel{convergence}{{4}{12}{Contour of the local and global optimization at a site. The brown concentric circles indicate the perturbation of the local cost to learn the global cost. The path traced by the yellow ochre colored circles indicate a possible optimization path if only the local cost were considered. The green colored circles show the path if the global cost were to be optimized. \relax }{figure.4}{}}
\bibstyle{spmpsci}
\bibdata{fs}
\bibcite{rakesh1995}{1}
\bibcite{antunes2003}{2}
\bibcite{bitSpade}{3}
\bibcite{gehrke2002}{4}
\bibcite{Benveniste_90}{5}
\bibcite{Bertsekas_97}{6}
\bibcite{bottou-98x}{7}
\bibcite{Bottou_05}{8}
\bibcite{Boyd_04}{9}
\bibcite{ChalamallaNSR08}{10}
\bibcite{Cybenko_89}{11}
\bibcite{Du_01a}{12}
\bibcite{Du_01b}{13}
\bibcite{rastogi1999}{14}
\bibcite{l1Reg}{15}
\bibcite{Hyers_52}{16}
\bibcite{JawanpuriaNR11}{17}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experimental Evaluation}{13}{figure.4}}
\newlabel{sec:expt}{{7}{13}{Experimental Evaluation\relax }{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Aims}{13}{figure.4}}
\newlabel{sec:exptaims}{{7.1}{13}{Aims\relax }{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Materials}{13}{figure.4}}
\newlabel{sec:exptmat}{{7.2}{13}{Materials\relax }{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Method}{13}{figure.4}}
\newlabel{sec:exptmeth}{{7.3}{13}{Method\relax }{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results and Discussion}{13}{figure.4}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{13}{figure.4}}
\bibcite{subseqGap2006}{18}
\bibcite{JoshiRS08}{19}
\bibcite{Kempe_03}{20}
\bibcite{nips2004}{21}
\bibcite{Larson_77}{22}
\bibcite{lecun-98a}{23}
\bibcite{NageshRCKDB12}{24}
\bibcite{NairSRK12}{25}
\bibcite{cvpr2007}{26}
\bibcite{Pales_02}{27}
\bibcite{pei2004}{28}
\bibcite{han2005}{29}
\bibcite{han2004}{30}
\bibcite{RamakrishnanJBS07}{31}
\bibcite{Amrita12}{32}
\bibcite{SpeciaSJRN09}{33}
\bibcite{Specia_09}{34}
\bibcite{SpeciaSRN06}{35}
\bibcite{Tsitsiklis_86}{36}
\bibcite{Vaidya_02}{37}
\bibcite{Varga_62}{38}
